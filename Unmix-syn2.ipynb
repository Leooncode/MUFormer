{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c70885-fe12-4bf9-9c18-dcd69947a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###syn-2###\n",
    "from utils.VCA import *\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import scipy.io as io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "L = 198\n",
    "P = 4\n",
    "nr1 = 50\n",
    "nc1 = 50\n",
    "T = 15\n",
    "\n",
    "End_path = './data/endmembers_vca_synth_ex2.mat'\n",
    "M0 = io.loadmat(End_path)\n",
    "M0 = torch.tensor(M0['M0']).float().to('cuda:0')\n",
    "\n",
    "\n",
    "def seed_everything(seed=999):\n",
    "    '''\n",
    "    设置整个开发环境的seed\n",
    "    :param seed:\n",
    "    :param device:\n",
    "    :return:\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()\n",
    "\n",
    "def endmember(HSI):\n",
    "    E_torch, _ = vca(HSI, P, snr_input=30)\n",
    "    return E_torch\n",
    "\n",
    "def deal_end(end):\n",
    "    end = end.unsqueeze(2).unsqueeze(3).float()\n",
    "    return end\n",
    "\n",
    "def np_to_torch(img_np):\n",
    "    '''Converts image in numpy.array to torch.Tensor.\n",
    "\n",
    "    From C x W x H [0..1] to  C x W x H [0..1]\n",
    "    '''\n",
    "    img_np.astype(np.float32)\n",
    "    return torch.from_numpy(img_np)\n",
    "\n",
    "def End_deal(path, device):\n",
    "    HSI = io.loadmat(path)\n",
    "    HSI = HSI['Y'].transpose([3, 0, 1, 2])\n",
    "    HSI = torch.tensor(HSI)\n",
    "    HSI_1 = HSI[0, :, :, :].reshape(L, -1)\n",
    "    HSI_2 = HSI[1, :, :, :].reshape(L, -1)\n",
    "    HSI_3 = HSI[2, :, :, :].reshape(L, -1)\n",
    "    HSI_4 = HSI[3, :, :, :].reshape(L, -1)\n",
    "    HSI_5 = HSI[4, :, :, :].reshape(L, -1)\n",
    "    HSI_6 = HSI[5, :, :, :].reshape(L, -1)\n",
    "    HSI_7 = HSI[6, :, :, :].reshape(L, -1)\n",
    "    HSI_8 = HSI[7, :, :, :].reshape(L, -1)\n",
    "    HSI_9 = HSI[8, :, :, :].reshape(L, -1)\n",
    "    HSI_10 = HSI[9, :, :, :].reshape(L, -1)\n",
    "    HSI_11 = HSI[10, :, :, :].reshape(L, -1)\n",
    "    HSI_12 = HSI[11, :, :, :].reshape(L, -1)\n",
    "    HSI_13 = HSI[12, :, :, :].reshape(L, -1)\n",
    "    HSI_14 = HSI[13, :, :, :].reshape(L, -1)\n",
    "    HSI_15 = HSI[14, :, :, :].reshape(L, -1)\n",
    "\n",
    "    \n",
    "    Endmember_1 = endmember(HSI_1)\n",
    "    Endmember_2 = endmember(HSI_2)\n",
    "    Endmember_3 = endmember(HSI_3)\n",
    "    Endmember_4 = endmember(HSI_4)\n",
    "    Endmember_5 = endmember(HSI_5)\n",
    "    Endmember_6 = endmember(HSI_6)\n",
    "    Endmember_7 = endmember(HSI_7)\n",
    "    Endmember_8 = endmember(HSI_8)\n",
    "    Endmember_9 = endmember(HSI_9)\n",
    "    Endmember_10 = endmember(HSI_10)\n",
    "    Endmember_11 = endmember(HSI_11)\n",
    "    Endmember_12 = endmember(HSI_12)\n",
    "    Endmember_13 = endmember(HSI_13)\n",
    "    Endmember_14 = endmember(HSI_14)\n",
    "    Endmember_15 = endmember(HSI_15)\n",
    "\n",
    "    \n",
    "    Endmember_1 = np_to_torch(Endmember_1)\n",
    "    Endmember_2 = np_to_torch(Endmember_2)\n",
    "    Endmember_3 = np_to_torch(Endmember_3)\n",
    "    Endmember_4 = np_to_torch(Endmember_4)\n",
    "    Endmember_5 = np_to_torch(Endmember_5)\n",
    "    Endmember_6 = np_to_torch(Endmember_6)\n",
    "    Endmember_7 = np_to_torch(Endmember_7)\n",
    "    Endmember_8 = np_to_torch(Endmember_8)\n",
    "    Endmember_9 = np_to_torch(Endmember_9)\n",
    "    Endmember_10 = np_to_torch(Endmember_10)\n",
    "    Endmember_11 = np_to_torch(Endmember_11)\n",
    "    Endmember_12 = np_to_torch(Endmember_12)\n",
    "    Endmember_13 = np_to_torch(Endmember_13)\n",
    "    Endmember_14 = np_to_torch(Endmember_14)\n",
    "    Endmember_15 = np_to_torch(Endmember_15)\n",
    "    \n",
    "    Endmember_1 = Endmember_1.to(device)\n",
    "    Endmember_2 = Endmember_2.to(device)\n",
    "    Endmember_3 = Endmember_3.to(device)\n",
    "    Endmember_4 = Endmember_4.to(device)\n",
    "    Endmember_5 = Endmember_5.to(device)\n",
    "    Endmember_6 = Endmember_6.to(device)\n",
    "    Endmember_7 = Endmember_7.to(device)\n",
    "    Endmember_8 = Endmember_8.to(device)\n",
    "    Endmember_9 = Endmember_9.to(device)\n",
    "    Endmember_10 = Endmember_10.to(device)\n",
    "    Endmember_11 = Endmember_11.to(device)\n",
    "    Endmember_12 = Endmember_12.to(device)\n",
    "    Endmember_13 = Endmember_13.to(device)\n",
    "    Endmember_14 = Endmember_14.to(device)\n",
    "    Endmember_15 = Endmember_15.to(device)\n",
    "\n",
    "    # Endmember_1 = deal_end(Endmember_1)\n",
    "    # Endmember_2 = deal_end(Endmember_2)\n",
    "    # Endmember_3 = deal_end(Endmember_3)\n",
    "    # Endmember_4 = deal_end(Endmember_4)\n",
    "    # Endmember_5 = deal_end(Endmember_5)\n",
    "    # Endmember_6 = deal_end(Endmember_6)\n",
    "    # Endmember_7 = deal_end(Endmember_7)\n",
    "    # Endmember_8 = deal_end(Endmember_8)\n",
    "    # Endmember_9 = deal_end(Endmember_9)\n",
    "    # Endmember_10 = deal_end(Endmember_10)\n",
    "    # Endmember_11 = deal_end(Endmember_11)\n",
    "    # Endmember_12 = deal_end(Endmember_12)\n",
    "    # Endmember_13 = deal_end(Endmember_13)\n",
    "    # Endmember_14 = deal_end(Endmember_14)\n",
    "    # Endmember_15 = deal_end(Endmember_15)\n",
    "\n",
    "    Endmember_1[Endmember_1 < 0] = 0\n",
    "    Endmember_2[Endmember_2 < 0] = 0\n",
    "    Endmember_3[Endmember_3 < 0] = 0\n",
    "    Endmember_4[Endmember_4 < 0] = 0\n",
    "    Endmember_5[Endmember_5 < 0] = 0\n",
    "    Endmember_6[Endmember_6 < 0] = 0\n",
    "    Endmember_7[Endmember_7 < 0] = 0\n",
    "    Endmember_8[Endmember_8 < 0] = 0\n",
    "    Endmember_9[Endmember_9 < 0] = 0\n",
    "    Endmember_10[Endmember_10 < 0] = 0\n",
    "    Endmember_11[Endmember_11 < 0] = 0\n",
    "    Endmember_12[Endmember_12 < 0] = 0\n",
    "    Endmember_13[Endmember_13 < 0] = 0\n",
    "    Endmember_14[Endmember_14 < 0] = 0\n",
    "    Endmember_15[Endmember_15 < 0] = 0\n",
    "\n",
    "    HSI = HSI.contiguous().view(1, T, L, 50, 50).float()\n",
    "    HSI = HSI.to(device)\n",
    "    return Endmember_1, Endmember_2, Endmember_3, Endmember_4, Endmember_5, Endmember_6, Endmember_7, Endmember_8, Endmember_9, \\\n",
    "Endmember_10, Endmember_11, Endmember_12, Endmember_13, Endmember_14, Endmember_15, HSI\n",
    "\n",
    "class SAD(nn.Module):\n",
    "    def __init__(self, num_bands):\n",
    "        super(SAD, self).__init__()\n",
    "        self.num_bands = num_bands\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        input_norm = torch.sqrt(torch.bmm(inp.view(-1, 1, self.num_bands),\n",
    "                                          inp.view(-1, self.num_bands, 1)))\n",
    "        target_norm = torch.sqrt(torch.bmm(target.view(-1, 1, self.num_bands),\n",
    "                                           target.view(-1, self.num_bands, 1)))\n",
    "\n",
    "        summation = torch.bmm(inp.view(-1, 1, self.num_bands), target.view(-1, self.num_bands, 1))\n",
    "        angle = torch.acos(summation / (input_norm * target_norm))\n",
    "\n",
    "        return angle\n",
    "\n",
    "\n",
    "class NonZeroClipper(object):\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w.clamp_(1e-6, 1)\n",
    "\n",
    "def Nuclear_norm(inputs):\n",
    "    band, h, w = inputs.shape\n",
    "    input = torch.reshape(inputs, (band, h*w))\n",
    "    out = torch.norm(input, p='nuc')\n",
    "    return out\n",
    "\n",
    "def end_loss(HSI, End):\n",
    "    End = End.cpu()\n",
    "    O = torch.mean(HSI.view(L,nr1*nc1),1).view(L,1).cpu() # (224, 1)\n",
    "    B = torch.from_numpy(np.identity(P)).float()  #(3, 3)\n",
    "    loss_end = torch.norm(torch.mm(End, B.view((P, P))) - O, 'fro')**2\n",
    "    \n",
    "    return loss_end.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b442d0-d741-4bca-a5f5-25c0191be24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "input SNR = 30[dB]\n",
      "\n",
      "... Select the projective proj.\n",
      "Epoch: 0 | total loss: 0.0967 | reconstruction loss: 0.0787 | SAD loss: 0.0756 | loss_sparse: 0.0020 | end_loss: 0.0369\n",
      "Epoch: 20 | total loss: 0.0777 | reconstruction loss: 0.0633 | SAD loss: 0.0605 | loss_sparse: 0.0020 | end_loss: 0.0120\n",
      "Epoch: 40 | total loss: 0.0647 | reconstruction loss: 0.0598 | SAD loss: 0.0482 | loss_sparse: 0.0020 | end_loss: 0.0120\n",
      "Epoch: 60 | total loss: 0.0592 | reconstruction loss: 0.0575 | SAD loss: 0.0432 | loss_sparse: 0.0020 | end_loss: 0.0174\n",
      "Epoch: 80 | total loss: 0.0515 | reconstruction loss: 0.0535 | SAD loss: 0.0364 | loss_sparse: 0.0020 | end_loss: 0.0221\n",
      "Epoch: 100 | total loss: 0.0494 | reconstruction loss: 0.0517 | SAD loss: 0.0348 | loss_sparse: 0.0020 | end_loss: 0.0256\n",
      "Epoch: 120 | total loss: 0.0442 | reconstruction loss: 0.0475 | SAD loss: 0.0305 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 140 | total loss: 0.0425 | reconstruction loss: 0.0450 | SAD loss: 0.0294 | loss_sparse: 0.0020 | end_loss: 0.0315\n",
      "Epoch: 160 | total loss: 0.0410 | reconstruction loss: 0.0435 | SAD loss: 0.0283 | loss_sparse: 0.0020 | end_loss: 0.0332\n",
      "Epoch: 180 | total loss: 0.0398 | reconstruction loss: 0.0420 | SAD loss: 0.0274 | loss_sparse: 0.0020 | end_loss: 0.0331\n",
      "Epoch: 200 | total loss: 0.0391 | reconstruction loss: 0.0405 | SAD loss: 0.0271 | loss_sparse: 0.0020 | end_loss: 0.0334\n",
      "Epoch: 220 | total loss: 0.0383 | reconstruction loss: 0.0393 | SAD loss: 0.0267 | loss_sparse: 0.0020 | end_loss: 0.0334\n",
      "Epoch: 240 | total loss: 0.0377 | reconstruction loss: 0.0383 | SAD loss: 0.0262 | loss_sparse: 0.0020 | end_loss: 0.0327\n",
      "Epoch: 260 | total loss: 0.0369 | reconstruction loss: 0.0370 | SAD loss: 0.0258 | loss_sparse: 0.0020 | end_loss: 0.0321\n",
      "Epoch: 280 | total loss: 0.0366 | reconstruction loss: 0.0359 | SAD loss: 0.0257 | loss_sparse: 0.0020 | end_loss: 0.0313\n",
      "Epoch: 300 | total loss: 0.0528 | reconstruction loss: 0.0454 | SAD loss: 0.0398 | loss_sparse: 0.0020 | end_loss: 0.0275\n",
      "Epoch: 320 | total loss: 0.0394 | reconstruction loss: 0.0386 | SAD loss: 0.0279 | loss_sparse: 0.0020 | end_loss: 0.0287\n",
      "Epoch: 340 | total loss: 0.0397 | reconstruction loss: 0.0388 | SAD loss: 0.0282 | loss_sparse: 0.0020 | end_loss: 0.0302\n",
      "Epoch: 360 | total loss: 0.0396 | reconstruction loss: 0.0387 | SAD loss: 0.0281 | loss_sparse: 0.0020 | end_loss: 0.0307\n",
      "Epoch: 380 | total loss: 0.0391 | reconstruction loss: 0.0383 | SAD loss: 0.0277 | loss_sparse: 0.0020 | end_loss: 0.0304\n",
      "Epoch: 400 | total loss: 0.0380 | reconstruction loss: 0.0369 | SAD loss: 0.0269 | loss_sparse: 0.0020 | end_loss: 0.0302\n",
      "Epoch: 420 | total loss: 0.0376 | reconstruction loss: 0.0362 | SAD loss: 0.0267 | loss_sparse: 0.0020 | end_loss: 0.0302\n",
      "Epoch: 440 | total loss: 0.0371 | reconstruction loss: 0.0355 | SAD loss: 0.0264 | loss_sparse: 0.0020 | end_loss: 0.0300\n",
      "Epoch: 460 | total loss: 0.0367 | reconstruction loss: 0.0349 | SAD loss: 0.0261 | loss_sparse: 0.0020 | end_loss: 0.0300\n",
      "Epoch: 480 | total loss: 0.0364 | reconstruction loss: 0.0344 | SAD loss: 0.0259 | loss_sparse: 0.0020 | end_loss: 0.0301\n",
      "Epoch: 500 | total loss: 0.0361 | reconstruction loss: 0.0339 | SAD loss: 0.0257 | loss_sparse: 0.0020 | end_loss: 0.0299\n",
      "Epoch: 520 | total loss: 0.0359 | reconstruction loss: 0.0337 | SAD loss: 0.0256 | loss_sparse: 0.0020 | end_loss: 0.0298\n",
      "Epoch: 540 | total loss: 0.0355 | reconstruction loss: 0.0332 | SAD loss: 0.0254 | loss_sparse: 0.0020 | end_loss: 0.0297\n",
      "Epoch: 560 | total loss: 0.0356 | reconstruction loss: 0.0331 | SAD loss: 0.0254 | loss_sparse: 0.0020 | end_loss: 0.0296\n",
      "Epoch: 580 | total loss: 0.0353 | reconstruction loss: 0.0328 | SAD loss: 0.0252 | loss_sparse: 0.0020 | end_loss: 0.0294\n",
      "Epoch: 600 | total loss: 0.0351 | reconstruction loss: 0.0325 | SAD loss: 0.0251 | loss_sparse: 0.0020 | end_loss: 0.0294\n",
      "Epoch: 620 | total loss: 0.0349 | reconstruction loss: 0.0323 | SAD loss: 0.0250 | loss_sparse: 0.0020 | end_loss: 0.0294\n",
      "Epoch: 640 | total loss: 0.0347 | reconstruction loss: 0.0321 | SAD loss: 0.0248 | loss_sparse: 0.0020 | end_loss: 0.0293\n",
      "Epoch: 660 | total loss: 0.0354 | reconstruction loss: 0.0322 | SAD loss: 0.0255 | loss_sparse: 0.0020 | end_loss: 0.0289\n",
      "Epoch: 680 | total loss: 0.0358 | reconstruction loss: 0.0326 | SAD loss: 0.0258 | loss_sparse: 0.0020 | end_loss: 0.0290\n",
      "Epoch: 700 | total loss: 0.0344 | reconstruction loss: 0.0316 | SAD loss: 0.0247 | loss_sparse: 0.0020 | end_loss: 0.0290\n",
      "Epoch: 720 | total loss: 0.0343 | reconstruction loss: 0.0315 | SAD loss: 0.0245 | loss_sparse: 0.0020 | end_loss: 0.0290\n",
      "Epoch: 740 | total loss: 0.0342 | reconstruction loss: 0.0314 | SAD loss: 0.0244 | loss_sparse: 0.0020 | end_loss: 0.0290\n",
      "Epoch: 760 | total loss: 0.0341 | reconstruction loss: 0.0313 | SAD loss: 0.0244 | loss_sparse: 0.0020 | end_loss: 0.0289\n",
      "Epoch: 780 | total loss: 0.0339 | reconstruction loss: 0.0311 | SAD loss: 0.0243 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 800 | total loss: 0.0339 | reconstruction loss: 0.0310 | SAD loss: 0.0243 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 820 | total loss: 0.0337 | reconstruction loss: 0.0308 | SAD loss: 0.0241 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 840 | total loss: 0.0336 | reconstruction loss: 0.0307 | SAD loss: 0.0241 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 860 | total loss: 0.0335 | reconstruction loss: 0.0306 | SAD loss: 0.0240 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 880 | total loss: 0.0334 | reconstruction loss: 0.0305 | SAD loss: 0.0239 | loss_sparse: 0.0020 | end_loss: 0.0288\n",
      "Epoch: 900 | total loss: 0.0333 | reconstruction loss: 0.0304 | SAD loss: 0.0238 | loss_sparse: 0.0020 | end_loss: 0.0287\n",
      "Epoch: 920 | total loss: 0.0332 | reconstruction loss: 0.0303 | SAD loss: 0.0238 | loss_sparse: 0.0020 | end_loss: 0.0287\n",
      "Epoch: 940 | total loss: 0.0335 | reconstruction loss: 0.0306 | SAD loss: 0.0240 | loss_sparse: 0.0020 | end_loss: 0.0284\n",
      "Epoch: 960 | total loss: 0.0330 | reconstruction loss: 0.0301 | SAD loss: 0.0236 | loss_sparse: 0.0020 | end_loss: 0.0284\n",
      "Epoch: 980 | total loss: 0.0328 | reconstruction loss: 0.0299 | SAD loss: 0.0235 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1000 | total loss: 0.0328 | reconstruction loss: 0.0298 | SAD loss: 0.0234 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1020 | total loss: 0.0327 | reconstruction loss: 0.0298 | SAD loss: 0.0234 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1040 | total loss: 0.0326 | reconstruction loss: 0.0297 | SAD loss: 0.0233 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1060 | total loss: 0.0326 | reconstruction loss: 0.0296 | SAD loss: 0.0233 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1080 | total loss: 0.0326 | reconstruction loss: 0.0296 | SAD loss: 0.0234 | loss_sparse: 0.0020 | end_loss: 0.0286\n",
      "Epoch: 1100 | total loss: 0.0324 | reconstruction loss: 0.0295 | SAD loss: 0.0232 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1120 | total loss: 0.0323 | reconstruction loss: 0.0294 | SAD loss: 0.0231 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1140 | total loss: 0.0322 | reconstruction loss: 0.0293 | SAD loss: 0.0230 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1160 | total loss: 0.0321 | reconstruction loss: 0.0292 | SAD loss: 0.0229 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1180 | total loss: 0.0320 | reconstruction loss: 0.0291 | SAD loss: 0.0229 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1200 | total loss: 0.0320 | reconstruction loss: 0.0290 | SAD loss: 0.0228 | loss_sparse: 0.0020 | end_loss: 0.0285\n",
      "Epoch: 1220 | total loss: 0.0319 | reconstruction loss: 0.0290 | SAD loss: 0.0228 | loss_sparse: 0.0020 | end_loss: 0.0283\n",
      "Epoch: 1240 | total loss: 0.0325 | reconstruction loss: 0.0292 | SAD loss: 0.0233 | loss_sparse: 0.0020 | end_loss: 0.0280\n",
      "Epoch: 1260 | total loss: 0.0319 | reconstruction loss: 0.0285 | SAD loss: 0.0229 | loss_sparse: 0.0020 | end_loss: 0.0275\n",
      "Epoch: 1280 | total loss: 0.0320 | reconstruction loss: 0.0288 | SAD loss: 0.0229 | loss_sparse: 0.0020 | end_loss: 0.0273\n",
      "Epoch: 1300 | total loss: 0.0317 | reconstruction loss: 0.0287 | SAD loss: 0.0226 | loss_sparse: 0.0020 | end_loss: 0.0271\n",
      "Epoch: 1320 | total loss: 0.0312 | reconstruction loss: 0.0283 | SAD loss: 0.0223 | loss_sparse: 0.0020 | end_loss: 0.0277\n",
      "Epoch: 1340 | total loss: 0.0317 | reconstruction loss: 0.0286 | SAD loss: 0.0227 | loss_sparse: 0.0020 | end_loss: 0.0277\n",
      "Epoch: 1360 | total loss: 0.0316 | reconstruction loss: 0.0284 | SAD loss: 0.0226 | loss_sparse: 0.0020 | end_loss: 0.0278\n",
      "Epoch: 1380 | total loss: 0.0316 | reconstruction loss: 0.0285 | SAD loss: 0.0226 | loss_sparse: 0.0020 | end_loss: 0.0279\n",
      "Epoch: 1400 | total loss: 0.0315 | reconstruction loss: 0.0284 | SAD loss: 0.0226 | loss_sparse: 0.0020 | end_loss: 0.0280\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.dealmat import *\n",
    "from Muformer_pytorch.muformer_syn2 import Muformer\n",
    "from loss import sad_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_syn2 import plot_abundance\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.linalg import lstsq\n",
    "            \n",
    "def main():\n",
    "    path = './data/synth_dataset_ex2.mat'\n",
    "    lr = 0.01\n",
    "    sparse_decay, weight_decay = 4e-5, 4e-5\n",
    "    tv_lamda = 1e-3\n",
    "    epoch = 2000\n",
    "    L = 198\n",
    "    device = 'cuda:0'\n",
    "    T = 15\n",
    "    nr1 = 50\n",
    "    nc1 = 50\n",
    "    P = 4\n",
    "    beta = 0.25\n",
    "    gamma = 0.99\n",
    "    N = nr1 * nc1\n",
    "    iter_rec, loss_rec, x_rec = list(), list(), list()\n",
    "    train_losses = []\n",
    "    order = (0, 1, 2, 3)\n",
    "    \n",
    "    net = Muformer(dim = 600,image_width = 50,image_height = 50,patch_size = 10,channels=L,\n",
    "    num_frames = T,P = P,depth = 2,heads = 8,dim_head =  60,attn_dropout = 0.1,ff_dropout = 0.1)\n",
    "    \n",
    "    Endmember_1, Endmember_2, Endmember_3, Endmember_4, Endmember_5, Endmember_6, Endmember_7, \\\n",
    "    Endmember_8, Endmember_9, Endmember_10, Endmember_11, Endmember_12, Endmember_13, Endmember_14, Endmember_15, HSI = End_deal(path, device)\n",
    "\n",
    "    model_dict = net.state_dict()\n",
    "\n",
    "    model_dict['decoder1.0.weight'] = Endmember_1\n",
    "    model_dict['decoder2.0.weight'] = Endmember_2\n",
    "    model_dict['decoder3.0.weight'] = Endmember_3\n",
    "    model_dict['decoder4.0.weight'] = Endmember_4\n",
    "    model_dict['decoder5.0.weight'] = Endmember_5\n",
    "    model_dict['decoder6.0.weight'] = Endmember_6\n",
    "    model_dict['decoder7.0.weight'] = Endmember_7\n",
    "    model_dict['decoder8.0.weight'] = Endmember_8\n",
    "    model_dict['decoder9.0.weight'] = Endmember_9\n",
    "    model_dict['decoder10.0.weight'] = Endmember_10\n",
    "    model_dict['decoder11.0.weight'] = Endmember_11\n",
    "    model_dict['decoder12.0.weight'] = Endmember_12\n",
    "    model_dict['decoder13.0.weight'] = Endmember_13\n",
    "    model_dict['decoder14.0.weight'] = Endmember_14\n",
    "    model_dict['decoder15.0.weight'] = Endmember_15\n",
    "\n",
    "    net.load_state_dict(model_dict)\n",
    "    \n",
    "    net.apply(net.weights_init)\n",
    "    net.to(device)\n",
    "    loss1 = nn.MSELoss(reduction='mean')\n",
    "    loss2 = SAD(L)\n",
    "    \n",
    "    class SparseKLloss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SparseKLloss, self).__init__()\n",
    "\n",
    "        def __call__(self, input, decay=sparse_decay):\n",
    "            input = torch.sum(input, 0, keepdim=True)\n",
    "            loss = Nuclear_norm(input)\n",
    "            return decay * loss\n",
    "        \n",
    "    criterionSparse = SparseKLloss()\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=40, gamma=0.94)\n",
    "    apply_clamp_inst1 = NonZeroClipper()\n",
    "    time_start = time.time()\n",
    "\n",
    "    for iter in range(epoch):\n",
    "        net.train()\n",
    "        abu, out = net(HSI)\n",
    "\n",
    "        re_loss1 = loss1(out[0, :, :, :], HSI[0, 0, :, :, :])\n",
    "        re_loss2 = loss1(out[1, :, :, :], HSI[0, 1, :, :, :])\n",
    "        re_loss3 = loss1(out[2, :, :, :], HSI[0, 2, :, :, :])\n",
    "        re_loss4 = loss1(out[3, :, :, :], HSI[0, 3, :, :, :])\n",
    "        re_loss5 = loss1(out[4, :, :, :], HSI[0, 4, :, :, :])\n",
    "        re_loss6 = loss1(out[5, :, :, :], HSI[0, 5, :, :, :])\n",
    "        re_loss7 = loss1(out[6, :, :, :], HSI[0, 6, :, :, :])\n",
    "        re_loss8 = loss1(out[7, :, :, :], HSI[0, 7, :, :, :])\n",
    "        re_loss9 = loss1(out[8, :, :, :], HSI[0, 8, :, :, :])\n",
    "        re_loss10 = loss1(out[9, :, :, :], HSI[0, 9, :, :, :])\n",
    "        re_loss11 = loss1(out[10, :, :, :], HSI[0, 10, :, :, :])\n",
    "        re_loss12 = loss1(out[11, :, :, :], HSI[0, 11, :, :, :])\n",
    "        re_loss13 = loss1(out[12, :, :, :], HSI[0, 12, :, :, :])\n",
    "        re_loss14 = loss1(out[13, :, :, :], HSI[0, 13, :, :, :])\n",
    "        re_loss15 = loss1(out[14, :, :, :], HSI[0, 14, :, :, :])\n",
    "\n",
    "        \n",
    "        loss_sad_1 = loss2(out[0, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 0, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_1 = loss_sad_1[~torch.isnan(loss_sad_1)]\n",
    "        loss_sad_1 = torch.sum(loss_sad_1)\n",
    "        \n",
    "        loss_sad_2 = loss2(out[1, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 1, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_2 = loss_sad_2[~torch.isnan(loss_sad_2)]\n",
    "        loss_sad_2 = torch.sum(loss_sad_2)\n",
    "        \n",
    "        loss_sad_3 = loss2(out[2, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 2, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_3 = loss_sad_3[~torch.isnan(loss_sad_3)]\n",
    "        loss_sad_3 = torch.sum(loss_sad_3)\n",
    "        \n",
    "        loss_sad_4 = loss2(out[3, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 3, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_4 = loss_sad_4[~torch.isnan(loss_sad_4)]\n",
    "        loss_sad_4 = torch.sum(loss_sad_4)\n",
    "        \n",
    "        loss_sad_5 = loss2(out[4, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 4, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_5 = loss_sad_5[~torch.isnan(loss_sad_5)]\n",
    "        loss_sad_5 = torch.sum(loss_sad_5)\n",
    "        \n",
    "        loss_sad_6 = loss2(out[5, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 5, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_6 = loss_sad_6[~torch.isnan(loss_sad_6)]\n",
    "        loss_sad_6 = torch.sum(loss_sad_6)\n",
    "        \n",
    "        loss_sad_7 = loss2(out[6, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 6, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_7 = loss_sad_7[~torch.isnan(loss_sad_7)]\n",
    "        loss_sad_7 = torch.sum(loss_sad_7)\n",
    "        \n",
    "        loss_sad_8 = loss2(out[7, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 7, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_8 = loss_sad_8[~torch.isnan(loss_sad_8)]\n",
    "        loss_sad_8 = torch.sum(loss_sad_8)\n",
    "        \n",
    "        loss_sad_9 = loss2(out[8, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 8, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_9 = loss_sad_9[~torch.isnan(loss_sad_9)]\n",
    "        loss_sad_9 = torch.sum(loss_sad_9)\n",
    "        \n",
    "        loss_sad_10 = loss2(out[9, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 9, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_10 = loss_sad_10[~torch.isnan(loss_sad_10)]\n",
    "        loss_sad_10 = torch.sum(loss_sad_10)\n",
    "        \n",
    "        loss_sad_11 = loss2(out[10, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 10, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_11 = loss_sad_11[~torch.isnan(loss_sad_11)]\n",
    "        loss_sad_11 = torch.sum(loss_sad_11)\n",
    "        \n",
    "        loss_sad_12 = loss2(out[11, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 11, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_12 = loss_sad_12[~torch.isnan(loss_sad_12)]\n",
    "        loss_sad_12 = torch.sum(loss_sad_12)\n",
    "        \n",
    "        loss_sad_13 = loss2(out[12, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 12, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_13 = loss_sad_13[~torch.isnan(loss_sad_13)]\n",
    "        loss_sad_13 = torch.sum(loss_sad_13)\n",
    "        \n",
    "        loss_sad_14 = loss2(out[13, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 13, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_14 = loss_sad_14[~torch.isnan(loss_sad_14)]\n",
    "        loss_sad_14 = torch.sum(loss_sad_14)\n",
    "        \n",
    "        loss_sad_15 = loss2(out[14, :, :, :].contiguous().view(1, L, -1).transpose(1, 2),\n",
    "                           HSI[0, 14, :, :, :].contiguous().view(1, L, -1).transpose(1, 2))\n",
    "        loss_sad_15 = loss_sad_15[~torch.isnan(loss_sad_15)]\n",
    "        loss_sad_15 = torch.sum(loss_sad_15)\n",
    "        \n",
    "        loss_sparse = torch.sum(criterionSparse(abu[0]) + criterionSparse(abu[1]) + criterionSparse(abu[2]) + \\\n",
    "                      criterionSparse(abu[3]) + criterionSparse(abu[4]) + criterionSparse(abu[5]) + criterionSparse(abu[6])\n",
    "                      + criterionSparse(abu[7]) + criterionSparse(abu[8]) + criterionSparse(abu[9]) + criterionSparse(abu[10])\n",
    "                      + criterionSparse(abu[11]) + criterionSparse(abu[12]) + criterionSparse(abu[13]) + criterionSparse(abu[14]))/T\n",
    "\n",
    "        loss_t1 = end_loss(HSI[0, 0, :, :, :], net.decoder1[0].weight)\n",
    "        loss_t2 = end_loss(HSI[0, 1, :, :, :], net.decoder2[0].weight)\n",
    "        loss_t3 = end_loss(HSI[0, 2, :, :, :], net.decoder3[0].weight)\n",
    "        loss_t4 = end_loss(HSI[0, 3, :, :, :], net.decoder4[0].weight)\n",
    "        loss_t5 = end_loss(HSI[0, 4, :, :, :], net.decoder5[0].weight)\n",
    "        loss_t6 = end_loss(HSI[0, 5, :, :, :], net.decoder6[0].weight)\n",
    "        loss_t7 = end_loss(HSI[0, 6, :, :, :], net.decoder7[0].weight)\n",
    "        loss_t8 = end_loss(HSI[0, 7, :, :, :], net.decoder8[0].weight)\n",
    "        loss_t9 = end_loss(HSI[0, 8, :, :, :], net.decoder9[0].weight)\n",
    "        loss_t10 = end_loss(HSI[0, 9, :, :, :], net.decoder10[0].weight)\n",
    "        loss_t11 = end_loss(HSI[0, 10, :, :, :], net.decoder11[0].weight)\n",
    "        loss_t12 = end_loss(HSI[0, 11, :, :, :], net.decoder12[0].weight)\n",
    "        loss_t13 = end_loss(HSI[0, 12, :, :, :], net.decoder13[0].weight)\n",
    "        loss_t14 = end_loss(HSI[0, 13, :, :, :], net.decoder14[0].weight)\n",
    "        loss_t15 = end_loss(HSI[0, 14, :, :, :], net.decoder15[0].weight)\n",
    "\n",
    "        total_end_loss = (loss_t1 + loss_t2 + loss_t3 + loss_t4 + loss_t5 + loss_t6\n",
    "                          + loss_t7 + loss_t8 + loss_t9 + loss_t10 + loss_t11 + loss_t12\n",
    "                          + loss_t13 + loss_t14 + loss_t15)/(T * L)\n",
    "        \n",
    "        re_loss = torch.sqrt((re_loss1 + re_loss2 + re_loss3 + re_loss4 + re_loss5 + re_loss6 + re_loss7 + \\\n",
    "                              re_loss8 + re_loss9 + re_loss10 + re_loss11 + re_loss12 + re_loss13 + re_loss14 + re_loss15)/ T )\n",
    "        loss_sad = (loss_sad_1 + loss_sad_2 + loss_sad_3 + loss_sad_4 + loss_sad_5 + loss_sad_6 + \\\n",
    "                    loss_sad_7 + loss_sad_8 + loss_sad_9 + loss_sad_10 + loss_sad_11 + loss_sad_12+ \\\n",
    "                    loss_sad_13 + loss_sad_14 + loss_sad_15)/(T * nc1 * nr1 * P)\n",
    "        total_loss = beta * re_loss + gamma * loss_sad +  0.004 * total_end_loss + loss_sparse\n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        net.decoder1.apply(apply_clamp_inst1)\n",
    "        net.decoder2.apply(apply_clamp_inst1)\n",
    "        net.decoder3.apply(apply_clamp_inst1)\n",
    "        net.decoder4.apply(apply_clamp_inst1)\n",
    "        net.decoder5.apply(apply_clamp_inst1)\n",
    "        net.decoder6.apply(apply_clamp_inst1)\n",
    "        net.decoder7.apply(apply_clamp_inst1)\n",
    "        net.decoder8.apply(apply_clamp_inst1)\n",
    "        net.decoder9.apply(apply_clamp_inst1)\n",
    "        net.decoder10.apply(apply_clamp_inst1)\n",
    "        net.decoder11.apply(apply_clamp_inst1)\n",
    "        net.decoder12.apply(apply_clamp_inst1)\n",
    "        net.decoder13.apply(apply_clamp_inst1)\n",
    "        net.decoder14.apply(apply_clamp_inst1)\n",
    "        net.decoder15.apply(apply_clamp_inst1)\n",
    "\n",
    "        \n",
    "        scheduler.step()\n",
    "        if iter % 20 == 0:\n",
    "            print('Epoch:', iter, '| total loss: %.4f' % total_loss.data, '| reconstruction loss: %.4f' % re_loss.data, \n",
    "                  '| SAD loss: %.4f' % loss_sad.data, '| loss_sparse: %.4f' % loss_sparse, '| end_loss: %.4f' % total_end_loss)\n",
    "        iter_rec.append(iter)\n",
    "        loss_rec.append(total_loss.cpu().detach().numpy())\n",
    "        \n",
    "    time_end = time.time()\n",
    "    total_time = time_end - time_start\n",
    "    print('The training lasts for {} seconds'.format(time_end - time_start))\n",
    "        \n",
    "    print('-------------------START EVAL---------------------')\n",
    "    net.eval()\n",
    "    abu_est, re_result = net(HSI)\n",
    "    \n",
    "    t1 = net.decoder1[0].weight\n",
    "    t2 = net.decoder2[0].weight\n",
    "    t3 = net.decoder3[0].weight\n",
    "    t4 = net.decoder4[0].weight\n",
    "    t5 = net.decoder5[0].weight\n",
    "    t6 = net.decoder6[0].weight\n",
    "    t7 = net.decoder7[0].weight\n",
    "    t8 = net.decoder8[0].weight\n",
    "    t9 = net.decoder9[0].weight\n",
    "    t10 = net.decoder10[0].weight\n",
    "    t11 = net.decoder11[0].weight\n",
    "    t12 = net.decoder12[0].weight\n",
    "    t13 = net.decoder13[0].weight\n",
    "    t14 = net.decoder14[0].weight\n",
    "    t15 = net.decoder15[0].weight\n",
    "\n",
    "    \n",
    "    abu = abu_est.contiguous().reshape(T, P, nr1, nc1)\n",
    "    #abu_est = abu / (torch.sum(abu, dim=1))\n",
    "    abu = abu.permute(2, 3, 1, 0).cpu().detach().numpy()\n",
    "    abu = abu[:, :, order, :]\n",
    "    # abu[:, :, :, 0] = abu[:, :, (1, 0, 3, 2), 0]\n",
    "    # abu[:, :, :, 1] = abu[:, :, (2, 0, 3, 1), 1]\n",
    "    # abu[:, :, :, 2] = abu[:, :, (3, 1, 2, 0), 2]\n",
    "    # abu[:, :, :, 5] = abu[:, :, (1, 0, 3, 2), 5]\n",
    "    # abu[:, :, :, 7] = abu[:, :, (3, 1, 0, 2), 7]\n",
    "    # abu[:, :, :, 8] = abu[:, :, (0, 3, 2, 1), 8]\n",
    "    # abu[:, :, :, 9] = abu[:, :, (1, 0, 3, 2), 9]\n",
    "    # abu[:, :, :, 10] = abu[:, :, (1, 0, 3, 2), 10]\n",
    "    # abu[:, :, :, 12] = abu[:, :, (1, 0, 3, 2), 12]\n",
    "\n",
    "    A_hat = abu\n",
    "    Y_hat = out.contiguous().view(T, L, -1)\n",
    "    Y_hat = Y_hat.cpu().detach().numpy()\n",
    "    Y_hat = Y_hat.transpose(2, 1, 0)\n",
    "    Mn_hat = torch.zeros((L, P, N, T))\n",
    "    \n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 0] = t1\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 1] = t2\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 2] = t3\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 3] = t4\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 4] = t5\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 5] = t6\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 6] = t7\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 7] = t8\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 8] = t9\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 9] = t10\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 10] = t11\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 11] = t12\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 12] = t13\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 13] = t14\n",
    "    for i in range(N):\n",
    "        Mn_hat[:, :, i, 14] = t15\n",
    "\n",
    "        \n",
    "    Mn_hat = Mn_hat[:, order, :, :]\n",
    "    # Mn_hat[:, :, :, 0] = Mn_hat[:, (1, 0, 3, 2), :, 0]\n",
    "    # Mn_hat[:, :, :, 1] = Mn_hat[:, (2, 0, 3, 1), :, 1]\n",
    "    # Mn_hat[:, :, :, 2] = Mn_hat[:, (3, 1, 2, 0), :, 2]\n",
    "    # Mn_hat[:, :, :, 5] = Mn_hat[:, (1, 0, 3, 2), :, 5]\n",
    "    # Mn_hat[:, :, :, 7] = Mn_hat[:, (3, 1, 0, 2), :, 7]\n",
    "    # Mn_hat[:, :, :, 8] = Mn_hat[:, (0, 3, 2, 1), :, 8]\n",
    "    # Mn_hat[:, :, :, 9] = Mn_hat[:, (1, 0, 3, 2), :, 9]\n",
    "    # Mn_hat[:, :, :, 10] = Mn_hat[:, (1, 0, 3, 2), :, 10]\n",
    "    # Mn_hat[:, :, :, 12] = Mn_hat[:, (1, 0, 3, 2), :, 12]\n",
    "\n",
    "    # Mn torch.Size([173, 3, 16500, 6])  'Mn_hat_VRNN': Mn_hat.cpu().detach().numpy()\n",
    "    \n",
    "    io.savemat('result_' + 'syn_2' + '.mat', \\\n",
    "                 {'A_hat_VRNN' : A_hat,\n",
    "                  'Y_hat_VRNN' : Y_hat,\n",
    "                  'Mn_hat_VRNN': Mn_hat.cpu().detach().numpy(),\n",
    "                 'time_VRNN': total_time})\n",
    "    \n",
    "    plot_abundance(abu=abu, iter_rec=iter_rec, loss_rec=loss_rec)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafdc5b-c8a6-4189-a159-93109e7b4b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muformer",
   "language": "python",
   "name": "muformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
